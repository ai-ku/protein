;;; CODE

hermite.pl: library of functions to compute hermite polynomial
coefficients, factorial, exponents, polynomial values etc.

two.pl: reads mode data from stdin and writes pair expectations for
later hermite calculation.  Requires hermite.pl.  Input format: 132
space separated dimensions on each line, number of lines = number of
time steps.  Output format: Tab separated h1, h2, m1, m2, exp where h1
and h2 are hermite degrees, m1 and m2 are the two modes, and exp is
the expectation.

one.pl: reads mode data from stdin and writes pair expectations for
later hermite calculation.  Requires hermite.pl.  Input format: same
as two.pl.  Output format: Tab separated h, m, exp where h is the
degree of the hermite, m is the index of the mode, and exp is the
expectation.

logf.pl: Library of functions (logf0, logf1, logf2) that compute log
likelihood for standard normal distribution with up to second order
hermite corrections.  All three functions take the data in an $x
vector (1-d array ref).  In addition logf1 needs the max hermite
degree $Nh and first degree expectations from hash $E1->{$h,$m}.
logf2 needs those and the second degree expectations from hash
$E2->{$h1,$h2,$m1,$m2}.  Any undef entries in the data array $x are to
be integrated out.

logf_test.pl: Specify the expectation files (outputs of one.pl and
two.pl) with -1 and -2.  Depending on which files are specified,
logf_test.pl will read the data from stdin and return its average
logf0, logf1, or logf2.


;;; 8967 EXPERIMENTS

8967b*: results of experiments with Mert's latest data (8967 time
steps).

8967b.dat: 8967x132 full data
8967b.tst.dat: 996x132 subsample
8967b.trn.dat: 7971x132 subsample (disjoint from tst)


;;; FILE TYPES

*.one: first degree expectations (output by "make *.one" using
one.pl).  Format: Tab separated h, m, exp where h is the degree of the
hermite, m is the index of the mode, and exp is the expectation.

*.two: second degree expectations (output by "make *.two" using
two.pl). Format: Tab separated h1, h2, m1, m2, exp where h1 and h2 are
hermite degrees, m1 and m2 are the two modes, and exp is the
expectation.

*.f0.*: pdf calculations with standard normal.  Output by "make *.f0".
*.f0.out: contains the average logf.
*.f0.err: contains the logf0 of each data point.

*.f1.*: pdf calculations with first degree Hermite corrections.
Output by "make *.f1".
*.f1.out: gives the H1 corrections used in calculation as well as the
final average logf1.
*.f1.err: contains the logf1 of each data point.

*.f2.*: pdf calculations with second degree Hermite corrections.
Output by "make *.f2".
*.f2.out: gives the H1 and H2 corrections used in calculation as well
as the final average logf2.
*.f2.err: contains the logf2 of each data point.

*.out, *.err: stdout and stderr output during the preparation of *.


;;; EXAMPLE

Assume raw data in foo.dat.  The above files would be generated by:

make foo.one > foo.one.out 2> foo.one.err
make foo.two > foo.two.out 2> foo.two.err
make foo.f0  > foo.f0.out  2> foo.f0.err
make foo.f1  > foo.f1.out  2> foo.f1.err
make foo.f2  > foo.f2.out  2> foo.f2.err


;;; KERNEL DENSITY ESTIMATION

Install the package np in R by typing install.package('np').  The
documentation is available at:
  http://cran.r-project.org/web/packages/np/np.pdf

Load the data files thus:
> train <- read.table('8967b.trn.dat')
> test <- read.table('8967b.tst.dat')

To create number sequences use c, seq, rep or ":".
Array indices start from 1.  Use sample to generate a random sample of
indices.  Here is how to subsample data:

train[sort(sample(1:nrow(train), 100)),]

> names(test) gives "V1" .. "V132"
> row.names(test) gives "1" .. "996"
> dim(test) gives (996 132)
> nrow(test) gives 996
> ncol(test) gives 996
> class(test) gives "data.frame"
> test[2] gives the second column of test (V2).
> test[2:4] gives the second thru fourth columns.
> test[[2]] gives the second column as a flat list.
> test[[2]][3] gives the third row of the second column.
> test[3,2] gives the third row, second column.
> test[1:3,1:2] gives the first three values of the first two columns.


Fast density model with default bandwidth calculation:

bw2 <- npudensbw(dat=train[,1:2], bwmethod="normal-reference")

Real bandwidth optimization is expensive

bw30 <- npudensbw(dat=train[sample(1:nrow(train),1000), 1:30], ftol=.01, tol=.01, nmulti=1)

t = 2 (s/1000)^2 d^2

where t is the number of seconds, s is the number of samples, d is the number of dimensions.

To create contour and perspective plots:

> x.seq <- seq(-3,3,length=100)
> y.seq <- seq(-3,3,length=100)
> data.eval <- expand.grid(x=x.seq, y=y.seq)
> fhat <- fitted(npudens(edat=data.eval, bws=bw2))
> f <- matrix(fhat, 100, 100)
> contour(x.seq, y.seq, f)
> persp(x.seq, y.seq, f)

Or create an interactive plot with:

> npplot(bw2)

Here is the best bandwidth so far achieved in 31 hours:

> system('date'); bw30 <- npudensbw(dat=trn[ , 1:30], ftol=.01, tol=.01, nmulti=1); system('date')
Sat Apr 17 19:22:05 EEST 2010
Mon Apr 19 02:42:01 EEST 2010
> bw30

Data (7971 observations, 30 variable(s)):

                     V1        V2        V3        V4        V5        V6
Bandwidth(s): 0.1158266 0.1735211 0.2049576 0.2256050 0.2985943 0.3536448
                     V7        V8        V9       V10      V11       V12
Bandwidth(s): 0.4143998 0.4322367 0.4920044 0.4283848 0.491907 0.5296707
                    V13       V14       V15       V16       V17       V18
Bandwidth(s): 0.4827285 0.6443203 0.6014523 0.6060795 0.6470107 0.7397163
                    V19       V20       V21       V22       V23       V24
Bandwidth(s): 0.7599039 0.7180294 0.8268029 0.8373491 0.8506736 0.8316038
                    V25       V26       V27       V28       V29      V30
Bandwidth(s): 0.8784527 0.7543386 0.8668307 0.8784526 0.8039685 0.919991

Bandwidth Selection Method: Maximum Likelihood Cross-Validation
Bandwidth Type: Fixed
Objective Function Value: 32.05061 (achieved on multistart 1)

Continuous Kernel Type: Second-Order Gaussian
No. Continuous Vars.: 30

> r <- sum(log(dmvnorm(x=tst[,31:ncol(tst)])));
> f <- npudens(tdat=trn[,1:30], edat=tst[,1:30], bws=bw30);
> (f$log_likelihood + r) / nrow(tst)
[1] -176.569
